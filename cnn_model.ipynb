{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMRqCkieqKEgOegajFwR109",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadNmatar/Adaptable-Handwritten-digit-Recognition/blob/main/cnn_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "2TZ9_ZJ9V_2A"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is about building a Convolutional Neural Networks for recognizing handwritten digit in 10 different languages.\n",
        "\n",
        "the dataset include the following languages:\n",
        "1) English\n",
        "2) Arabic\n",
        "3) Urdu\n",
        "4) Farsi\n",
        "5) Bangla\n",
        "6) Kannada\n",
        "7) Telugu\n",
        "8) Tibetan\n",
        "9) Devanagari"
      ],
      "metadata": {
        "id": "Tn0b4l2tjELx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PDjgZVMFcVkf"
      },
      "outputs": [],
      "source": [
        "#first we load the dataset\n",
        "from zipfile import ZipFile\n",
        "with ZipFile('./MNIST-MIX-all.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('dataset_folder')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(predicted_outcome, expected_outcome):\n",
        "    from sklearn import metrics\n",
        "    f1_score = metrics.f1_score(expected_outcome, predicted_outcome, average='weighted')\n",
        "    balanced_accuracy_score = metrics.balanced_accuracy_score(expected_outcome, predicted_outcome)\n",
        "    return f1_score, balanced_accuracy_score"
      ],
      "metadata": {
        "id": "Cpb3bIYSBs7m"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset_with_processing(input_train_test_fn, num_classes=10):\n",
        "    data = np.load(input_train_test_fn)\n",
        "    X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n",
        "\n",
        "    # split validation set\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    X_train, X_vali, y_train, y_vali = train_test_split(X_train, y_train, stratify=y_train, test_size=0.2)\n",
        "\n",
        "    X_train = np.array(X_train, dtype=\"float\") / 255.0\n",
        "    X_train = X_train.reshape(X_train.shape[0], 28, 28,1 )\n",
        "    print(X_train.shape)\n",
        "\n",
        "    X_vali = np.array(X_vali, dtype=\"float\") / 255.0\n",
        "    X_vali = X_vali.reshape(X_vali.shape[0], 28, 28,1 )\n",
        "    print(X_vali.shape)\n",
        "\n",
        "    X_test = np.array(X_test, dtype=\"float\") / 255.0\n",
        "    X_test = X_test.reshape(X_test.shape[0], 28, 28,1 )\n",
        "    print(X_test.shape)\n",
        "\n",
        "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "    y_vali = keras.utils.to_categorical(y_vali, num_classes)\n",
        "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "    return X_train, y_train, X_vali, y_vali, X_test, y_test"
      ],
      "metadata": {
        "id": "0at-kgEWBunH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def myGenerator(X_train, y_train, batch_size):\n",
        "    total_size = X_train.shape[0]\n",
        "\n",
        "    while True:\n",
        "        permutation = list(np.random.permutation(total_size))\n",
        "        for i in range(total_size//batch_size):\n",
        "            index = permutation[i * batch_size : (i + 1) * batch_size]\n",
        "            X_batch = X_train[index]\n",
        "            y_batch = y_train[index]\n",
        "\n",
        "            yield X_batch, y_batch"
      ],
      "metadata": {
        "id": "gSdUb2v8B3TK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combain all datasets together"
      ],
      "metadata": {
        "id": "rIrlBYDOUbWf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "\n",
        "def build_simple_cnn(input_shape, num_classes):\n",
        "    model = Sequential()\n",
        "\n",
        "    # First convolutional layer\n",
        "    model.add(Conv2D(32, (3, 3), padding=\"same\", input_shape=input_shape))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Second convolutional layer\n",
        "    model.add(Conv2D(64, (3, 3), padding=\"same\"))\n",
        "    model.add(Activation(\"relu\"))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "\n",
        "    # Flatten the output of the convolutional layers\n",
        "    model.add(Flatten())\n",
        "\n",
        "    # Fully connected layer\n",
        "    model.add(Dense(128))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    # Optional: Dropout to prevent overfitting (especially if using more data)\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Output layer with softmax for multi-class classification\n",
        "    model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4pSxh-G9UZl_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(X_train, y_train, X_vali, y_vali, input_shape, num_classes, loss, optimizer, metrics, checkpoint_dir, batch_size, epochs):\n",
        "    import os\n",
        "    if os.path.exists(checkpoint_dir) is False:\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    model = build_simple_cnn(input_shape, num_classes)\n",
        "\n",
        "    # Corrected indentation for checkpoint and es\n",
        "    checkpoint = ModelCheckpoint(checkpoint_dir + 'best.keras', monitor='val_accuracy', verbose=1, save_best_only=True, mode='auto')\n",
        "    #es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=30)\n",
        "\n",
        "\n",
        "    #Compile Keras Model\n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=metrics)\n",
        "\n",
        "    datagen = myGenerator(X_train, y_train, batch_size)\n",
        "\n",
        "    history = model.fit(datagen,\n",
        "                        steps_per_epoch = len(X_train) // batch_size,\n",
        "                        epochs=epochs,\n",
        "                        callbacks=[checkpoint],\n",
        "                        verbose=2,\n",
        "                        validation_data=(X_vali, y_vali))\n",
        "\n",
        "    plt.plot(history.history['loss'], label='train')\n",
        "    plt.plot(history.history['val_loss'], label='validation')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "kd5b1UU-C-NO"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_fn(X_test, y_test, input_shape, num_classes, loss, optimizer, metrics, pretrained_weights):\n",
        "    model = build_simple_cnn(input_shape, num_classes)\n",
        "\n",
        "    # Compile Keras Model\n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=metrics)\n",
        "\n",
        "    # load weights\n",
        "    model.load_weights(pretrained_weights)\n",
        "\n",
        "    score = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    testPredict = model.predict(X_test)\n",
        "    y_test_tmp = np.argmax(y_test, axis=1)\n",
        "    y_pred_tmp = np.argmax(testPredict, axis=1)\n",
        "    f1_score, balanced_accuracy_score = evaluate(y_pred_tmp, y_test_tmp)\n",
        "\n",
        "    return score[1], f1_score, balanced_accuracy_score  # return accuracies"
      ],
      "metadata": {
        "id": "qmeHgyMhDAhq"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_rows, img_cols = 28 , 28\n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "num_classes = 100\n",
        "loss = 'categorical_crossentropy'\n",
        "optimizer = Adam(learning_rate=0.01, clipnorm=0.001)\n",
        "metrics = ['accuracy']\n",
        "batch_size = 128\n",
        "epochs = 200\n",
        "\n",
        "checkpoint_dir = './MNIST_MIX_Trained_Model/'\n",
        "input_train_test_fn = './dataset_folder/MNIST_MIX_train_test.npz'\n",
        "\n",
        "# load data\n",
        "X_train, y_train, X_vali, y_vali, X_test, y_test = load_dataset_with_processing(input_train_test_fn, num_classes=100)\n",
        "\n"
      ],
      "metadata": {
        "id": "E70NGUL5NqaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_fn(X_train, y_train, X_vali, y_vali, input_shape, num_classes, loss, optimizer, metrics, checkpoint_dir, batch_size, epochs)"
      ],
      "metadata": {
        "id": "3NOo6wHGUIk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_weights = './MNIST_MIX_Trained_Model/best.keras'\n",
        "accuracies = test_fn(X_test, y_test, input_shape, num_classes, loss, optimizer, metrics, pretrained_weights)\n",
        "print('Accuracy: ', accuracies[0])\n",
        "print('Weighted F1: ', accuracies[1])\n",
        "print('Balanced Accuracy: ', accuracies[2])"
      ],
      "metadata": {
        "id": "fxkx5KvdY2NK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}